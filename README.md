这是一个新颖但**非常吃算力**的想法.

众所周知,让AI用自己生成的数据训练自己,过不了多久它就会崩溃.

为什么?因为AI生成的数据不能完全反映真实数据的模式,或者说,不够好.

而我们又知道,AI训练一直都在致力于让模型的输出变好.

所以----

**让AI用自己生成的数据训练自己,直到自己崩溃,崩溃前训练的轮数就可以代表AI是输出好不好**

从这个想法中,或许我们还可以提炼出一个相对轻量的测试方法:

1. 用训练集A训练出模型A
2. 用模型A生成**与训练集A不同的**训练集B
3. 用训练集B训练出模型B
4. 用模型B在训练集A上的表现作为模型A的指标

这很考验模型的泛化能力

-----------------------------------

注:

1. 虽然上面说用自己的数据训练自己,但通常是拿生成的数据用同样的超参数新训练一个模型

2. 我在这个项目里写了一个很简陋的抽象类,用于用上面的方法评估AI

3. 通常来说,让AI生成的训练数据一定是它之前没见过的,这样才能评估其泛化能力

4. 如何定义一个AI是否崩溃?反正不能用另一个AI判断,因为AI的判断通常对于那些好像崩了又好像没崩的模型不太公平

5. 我们知道了一个模型好不好,然后呢?那这个指标当损失函数算力肯定跟不上.或许可以拿这个指标调超参数?